{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(X_train, X_test,MAX_NB_WORDS=75000):\n",
    "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "    print(\"tf-idf with\",str(np.array(X_train).shape[1]),\"features\")\n",
    "    return (X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text classification based Deep Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):\n",
    "    \"\"\"\n",
    "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
    "    Build Deep neural networks Model for text classification\n",
    "    Shape is input feature space\n",
    "    nClasses is number of classes\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    nLayers = 4 # number of  hidden layer\n",
    "\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aljazeera Arabic corpus with 5 classes ( 'Art', 'Economic', 'Politics', 'Science', 'Sport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(r\"aji-Arabic_corpus.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Corpus['text'],Corpus['targe'],test_size=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 45917 features\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/14\n",
      " - 19s - loss: 1.6084 - accuracy: 0.2167 - val_loss: 1.5998 - val_accuracy: 0.1933\n",
      "Epoch 2/14\n",
      " - 15s - loss: 1.5774 - accuracy: 0.2225 - val_loss: 1.4649 - val_accuracy: 0.3133\n",
      "Epoch 3/14\n",
      " - 15s - loss: 1.3232 - accuracy: 0.4333 - val_loss: 0.8744 - val_accuracy: 0.7767\n",
      "Epoch 4/14\n",
      " - 15s - loss: 0.5515 - accuracy: 0.8292 - val_loss: 0.2701 - val_accuracy: 0.9100\n",
      "Epoch 5/14\n",
      " - 13s - loss: 0.1694 - accuracy: 0.9467 - val_loss: 0.2870 - val_accuracy: 0.9433\n",
      "Epoch 6/14\n",
      " - 14s - loss: 0.0497 - accuracy: 0.9883 - val_loss: 0.1794 - val_accuracy: 0.9700\n",
      "Epoch 7/14\n",
      " - 32s - loss: 0.0154 - accuracy: 0.9967 - val_loss: 0.2293 - val_accuracy: 0.9567\n",
      "Epoch 8/14\n",
      " - 19s - loss: 0.0140 - accuracy: 0.9967 - val_loss: 0.2060 - val_accuracy: 0.9667\n",
      "Epoch 9/14\n",
      " - 20s - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.1944 - val_accuracy: 0.9767\n",
      "Epoch 10/14\n",
      " - 13s - loss: 4.3540e-04 - accuracy: 1.0000 - val_loss: 0.2033 - val_accuracy: 0.9733\n",
      "Epoch 11/14\n",
      " - 14s - loss: 4.3947e-04 - accuracy: 1.0000 - val_loss: 0.2124 - val_accuracy: 0.9667\n",
      "Epoch 12/14\n",
      " - 13s - loss: 6.9476e-04 - accuracy: 1.0000 - val_loss: 0.2066 - val_accuracy: 0.9700\n",
      "Epoch 13/14\n",
      " - 14s - loss: 2.8735e-04 - accuracy: 1.0000 - val_loss: 0.2065 - val_accuracy: 0.9667\n",
      "Epoch 14/14\n",
      " - 13s - loss: 4.0460e-04 - accuracy: 1.0000 - val_loss: 0.2025 - val_accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f80bd422610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf,X_test_tfidf = TFIDF(X_train,X_test)\n",
    "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 5)\n",
    "model_DNN.fit(X_train_tfidf, y_train,\n",
    "                              validation_data=(X_test_tfidf, y_test),\n",
    "                              epochs=14,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        62\n",
      "           1       0.98      0.98      0.98        58\n",
      "           2       0.94      0.97      0.95        65\n",
      "           3       0.96      0.98      0.97        56\n",
      "           4       1.00      1.00      1.00        59\n",
      "\n",
      "    accuracy                           0.98       300\n",
      "   macro avg       0.98      0.98      0.98       300\n",
      "weighted avg       0.98      0.98      0.98       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_TEXT_LENGTH=46004\n",
    "Prediction = np.argmax(model_DNN.predict(X_test_tfidf[:, :MAX_TEXT_LENGTH]),axis=1)\n",
    "\n",
    "print(metrics.classification_report(y_test, Prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
