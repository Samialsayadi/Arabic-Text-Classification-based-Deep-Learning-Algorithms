{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout, Dense, GRU, Embedding,LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text to word embedding (Using Google Vector 50 Dim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\"cc.ar.50.vec\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text Classification based RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_RCNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50):\n",
    "\n",
    "    kernel_size = 2\n",
    "    filters = 256\n",
    "    pool_size = 2\n",
    "    gru_node = 256\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    \n",
    "    model.add(LSTM(gru_node, recurrent_dropout=0.2))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(nclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(r\"aji-Arabic_corpus.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Corpus['text'],Corpus['targe'],test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54193 unique tokens.\n",
      "(1500, 500)\n",
      "Total 1999990 word vectors.\n"
     ]
    }
   ],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 50)           2709700   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 499, 256)          25856     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 249, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 248, 256)          131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 124, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 123, 256)          131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 61, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 60, 256)           131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 5,364,937\n",
      "Trainable params: 5,364,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/15\n",
      " - 111s - loss: 1.6106 - accuracy: 0.2058 - val_loss: 1.6136 - val_accuracy: 0.2333\n",
      "Epoch 2/15\n",
      " - 59s - loss: 1.5923 - accuracy: 0.2475 - val_loss: 1.5221 - val_accuracy: 0.2867\n",
      "Epoch 3/15\n",
      " - 65s - loss: 1.3526 - accuracy: 0.3625 - val_loss: 1.1222 - val_accuracy: 0.4300\n",
      "Epoch 4/15\n",
      " - 69s - loss: 1.2235 - accuracy: 0.4375 - val_loss: 1.1205 - val_accuracy: 0.4633\n",
      "Epoch 5/15\n",
      " - 60s - loss: 0.9173 - accuracy: 0.5483 - val_loss: 0.8999 - val_accuracy: 0.5433\n",
      "Epoch 6/15\n",
      " - 61s - loss: 0.8031 - accuracy: 0.5600 - val_loss: 0.9647 - val_accuracy: 0.5100\n",
      "Epoch 7/15\n",
      " - 56s - loss: 0.6423 - accuracy: 0.5683 - val_loss: 0.6944 - val_accuracy: 0.6067\n",
      "Epoch 8/15\n",
      " - 56s - loss: 0.5825 - accuracy: 0.6425 - val_loss: 0.7817 - val_accuracy: 0.6033\n",
      "Epoch 9/15\n",
      " - 72s - loss: 0.5530 - accuracy: 0.6708 - val_loss: 0.7706 - val_accuracy: 0.6600\n",
      "Epoch 10/15\n",
      " - 62s - loss: 0.4527 - accuracy: 0.7542 - val_loss: 1.0140 - val_accuracy: 0.6300\n",
      "Epoch 11/15\n",
      " - 80s - loss: 0.3817 - accuracy: 0.8342 - val_loss: 0.8234 - val_accuracy: 0.7500\n",
      "Epoch 12/15\n",
      " - 60s - loss: 0.2694 - accuracy: 0.8800 - val_loss: 0.7107 - val_accuracy: 0.7800\n",
      "Epoch 13/15\n",
      " - 61s - loss: 0.1841 - accuracy: 0.9242 - val_loss: 0.7019 - val_accuracy: 0.8367\n",
      "Epoch 14/15\n",
      " - 152s - loss: 0.1456 - accuracy: 0.9500 - val_loss: 0.9007 - val_accuracy: 0.7400\n",
      "Epoch 15/15\n",
      " - 63s - loss: 0.0568 - accuracy: 0.9792 - val_loss: 0.6993 - val_accuracy: 0.8000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.86        67\n",
      "           1       0.68      0.81      0.74        57\n",
      "           2       0.70      0.90      0.79        58\n",
      "           3       0.72      0.52      0.60        60\n",
      "           4       1.00      0.98      0.99        58\n",
      "\n",
      "    accuracy                           0.80       300\n",
      "   macro avg       0.81      0.80      0.80       300\n",
      "weighted avg       0.81      0.80      0.80       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_RCNN = Build_Model_RCNN_Text(word_index,embeddings_index, 5)\n",
    "\n",
    "\n",
    "model_RCNN.summary()\n",
    "\n",
    "model_RCNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=15,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_RCNN.predict(X_test_Glove)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoreNLP, spaCy, Flair, huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 50)           2709700   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 499, 256)          25856     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 249, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 248, 256)          131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 124, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 123, 256)          131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 61, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 60, 256)           131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 5,364,937\n",
      "Trainable params: 5,364,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      " - 81s - loss: 1.6157 - accuracy: 0.1950 - val_loss: 1.6107 - val_accuracy: 0.1800\n",
      "Epoch 2/30\n",
      " - 62s - loss: 1.6074 - accuracy: 0.2092 - val_loss: 1.5848 - val_accuracy: 0.3067\n",
      "Epoch 3/30\n",
      " - 56s - loss: 1.4853 - accuracy: 0.3092 - val_loss: 1.2407 - val_accuracy: 0.4000\n",
      "Epoch 4/30\n",
      " - 66s - loss: 1.2244 - accuracy: 0.4092 - val_loss: 1.1685 - val_accuracy: 0.4167\n",
      "Epoch 5/30\n",
      " - 57s - loss: 1.0457 - accuracy: 0.4892 - val_loss: 1.3372 - val_accuracy: 0.4167\n",
      "Epoch 6/30\n",
      " - 58s - loss: 0.9818 - accuracy: 0.5533 - val_loss: 0.7348 - val_accuracy: 0.6233\n",
      "Epoch 7/30\n",
      " - 59s - loss: 0.6671 - accuracy: 0.6283 - val_loss: 0.6426 - val_accuracy: 0.6900\n",
      "Epoch 8/30\n",
      " - 57s - loss: 0.4231 - accuracy: 0.8125 - val_loss: 0.6118 - val_accuracy: 0.7567\n",
      "Epoch 9/30\n",
      " - 507s - loss: 0.2280 - accuracy: 0.9125 - val_loss: 0.7626 - val_accuracy: 0.7933\n",
      "Epoch 10/30\n",
      " - 69s - loss: 0.2247 - accuracy: 0.9125 - val_loss: 0.7276 - val_accuracy: 0.7833\n",
      "Epoch 11/30\n",
      " - 68s - loss: 0.0587 - accuracy: 0.9842 - val_loss: 0.9144 - val_accuracy: 0.7733\n",
      "Epoch 12/30\n",
      " - 63s - loss: 0.0314 - accuracy: 0.9933 - val_loss: 0.9917 - val_accuracy: 0.7800\n",
      "Epoch 13/30\n",
      " - 61s - loss: 0.0304 - accuracy: 0.9925 - val_loss: 1.0003 - val_accuracy: 0.8067\n",
      "Epoch 14/30\n",
      " - 64s - loss: 0.0422 - accuracy: 0.9842 - val_loss: 0.9902 - val_accuracy: 0.7933\n",
      "Epoch 15/30\n",
      " - 69s - loss: 0.0727 - accuracy: 0.9758 - val_loss: 0.9494 - val_accuracy: 0.7900\n",
      "Epoch 16/30\n",
      " - 73s - loss: 0.1954 - accuracy: 0.9292 - val_loss: 0.9951 - val_accuracy: 0.7500\n",
      "Epoch 17/30\n",
      " - 63s - loss: 0.1739 - accuracy: 0.9408 - val_loss: 0.9183 - val_accuracy: 0.7533\n",
      "Epoch 18/30\n",
      " - 60s - loss: 0.1404 - accuracy: 0.9517 - val_loss: 0.8378 - val_accuracy: 0.7800\n",
      "Epoch 19/30\n",
      " - 61s - loss: 0.0408 - accuracy: 0.9892 - val_loss: 0.7588 - val_accuracy: 0.7900\n",
      "Epoch 20/30\n",
      " - 83s - loss: 0.0205 - accuracy: 0.9958 - val_loss: 0.9039 - val_accuracy: 0.7933\n",
      "Epoch 21/30\n",
      " - 76s - loss: 0.0083 - accuracy: 0.9992 - val_loss: 0.9776 - val_accuracy: 0.8033\n",
      "Epoch 22/30\n",
      " - 70s - loss: 0.0078 - accuracy: 0.9983 - val_loss: 0.9656 - val_accuracy: 0.8100\n",
      "Epoch 23/30\n",
      " - 81s - loss: 0.0112 - accuracy: 0.9967 - val_loss: 1.1340 - val_accuracy: 0.7767\n",
      "Epoch 24/30\n",
      " - 76s - loss: 0.0042 - accuracy: 0.9992 - val_loss: 1.0517 - val_accuracy: 0.8167\n",
      "Epoch 25/30\n",
      " - 79s - loss: 0.0063 - accuracy: 0.9992 - val_loss: 1.1047 - val_accuracy: 0.7967\n",
      "Epoch 26/30\n",
      " - 104s - loss: 0.0031 - accuracy: 0.9992 - val_loss: 1.1661 - val_accuracy: 0.8100\n",
      "Epoch 27/30\n",
      " - 70s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 1.0557 - val_accuracy: 0.8200\n",
      "Epoch 28/30\n",
      " - 70s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 1.0791 - val_accuracy: 0.8067\n",
      "Epoch 29/30\n",
      " - 75s - loss: 6.0501e-04 - accuracy: 1.0000 - val_loss: 1.1886 - val_accuracy: 0.8100\n",
      "Epoch 30/30\n",
      " - 66s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.2084 - val_accuracy: 0.7933\n"
     ]
    }
   ],
   "source": [
    "model_RCNN = Build_Model_RCNN_Text(word_index,embeddings_index, 5)\n",
    "\n",
    "\n",
    "model_RCNN.summary()\n",
    "\n",
    "model_RCNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=30,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_RCNN.predict(X_test_Glove)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
