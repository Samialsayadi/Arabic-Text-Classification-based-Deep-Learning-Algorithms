{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout, Dense, GRU, Embedding,LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text to word embedding (Using Google Vector 50 Dim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\"cc.ar.50.vec\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text Classification based RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_RCNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50):\n",
    "\n",
    "    kernel_size = 2\n",
    "    filters = 256\n",
    "    pool_size = 2\n",
    "    gru_node = 256\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    \n",
    "    model.add(LSTM(gru_node, recurrent_dropout=0.2))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(nclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(r\"aji-Arabic_corpus.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Corpus['text'],Corpus['targe'],test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54193 unique tokens.\n",
      "(1500, 500)\n",
      "Total 1999990 word vectors.\n"
     ]
    }
   ],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 50)           2709700   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 499, 256)          25856     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 249, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 248, 256)          131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 124, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 123, 256)          131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 61, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 60, 256)           131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 5,364,937\n",
      "Trainable params: 5,364,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      " - 69s - loss: 1.6120 - accuracy: 0.2042 - val_loss: 1.6036 - val_accuracy: 0.2200\n",
      "Epoch 2/30\n",
      " - 50s - loss: 1.5878 - accuracy: 0.2567 - val_loss: 1.5787 - val_accuracy: 0.2400\n",
      "Epoch 3/30\n",
      " - 49s - loss: 1.4506 - accuracy: 0.3467 - val_loss: 1.2496 - val_accuracy: 0.3667\n",
      "Epoch 4/30\n",
      " - 50s - loss: 1.1001 - accuracy: 0.4625 - val_loss: 1.1508 - val_accuracy: 0.4167\n",
      "Epoch 5/30\n",
      " - 50s - loss: 0.9199 - accuracy: 0.5442 - val_loss: 0.8814 - val_accuracy: 0.5600\n",
      "Epoch 6/30\n",
      " - 50s - loss: 0.8560 - accuracy: 0.6125 - val_loss: 0.8869 - val_accuracy: 0.5667\n",
      "Epoch 7/30\n",
      " - 49s - loss: 0.7412 - accuracy: 0.6417 - val_loss: 0.8345 - val_accuracy: 0.5467\n",
      "Epoch 8/30\n",
      " - 49s - loss: 0.6064 - accuracy: 0.7000 - val_loss: 0.7320 - val_accuracy: 0.6033\n",
      "Epoch 9/30\n",
      " - 49s - loss: 0.5393 - accuracy: 0.7375 - val_loss: 0.6703 - val_accuracy: 0.6900\n",
      "Epoch 10/30\n",
      " - 48s - loss: 0.3958 - accuracy: 0.8033 - val_loss: 0.5519 - val_accuracy: 0.7867\n",
      "Epoch 11/30\n",
      " - 49s - loss: 0.2414 - accuracy: 0.9233 - val_loss: 0.5531 - val_accuracy: 0.8233\n",
      "Epoch 12/30\n",
      " - 49s - loss: 0.0951 - accuracy: 0.9700 - val_loss: 0.6677 - val_accuracy: 0.8367\n",
      "Epoch 13/30\n",
      " - 50s - loss: 0.0801 - accuracy: 0.9758 - val_loss: 0.6595 - val_accuracy: 0.8467\n",
      "Epoch 14/30\n",
      " - 50s - loss: 0.2484 - accuracy: 0.9200 - val_loss: 0.6794 - val_accuracy: 0.8100\n",
      "Epoch 15/30\n",
      " - 50s - loss: 0.1753 - accuracy: 0.9342 - val_loss: 0.4646 - val_accuracy: 0.8600\n",
      "Epoch 16/30\n",
      " - 49s - loss: 0.0628 - accuracy: 0.9908 - val_loss: 0.5800 - val_accuracy: 0.8300\n",
      "Epoch 17/30\n",
      " - 50s - loss: 0.0393 - accuracy: 0.9900 - val_loss: 0.5726 - val_accuracy: 0.8767\n",
      "Epoch 18/30\n",
      " - 49s - loss: 0.0203 - accuracy: 0.9967 - val_loss: 0.6165 - val_accuracy: 0.8800\n",
      "Epoch 19/30\n",
      " - 49s - loss: 0.0213 - accuracy: 0.9950 - val_loss: 0.6428 - val_accuracy: 0.8700\n",
      "Epoch 20/30\n",
      " - 49s - loss: 0.0174 - accuracy: 0.9967 - val_loss: 0.6674 - val_accuracy: 0.8733\n",
      "Epoch 21/30\n",
      " - 51s - loss: 0.0138 - accuracy: 0.9975 - val_loss: 0.6297 - val_accuracy: 0.8600\n",
      "Epoch 22/30\n",
      " - 49s - loss: 0.0115 - accuracy: 0.9975 - val_loss: 0.6297 - val_accuracy: 0.8600\n",
      "Epoch 23/30\n",
      " - 49s - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.6243 - val_accuracy: 0.8733\n",
      "Epoch 24/30\n",
      " - 49s - loss: 0.0082 - accuracy: 0.9992 - val_loss: 0.6635 - val_accuracy: 0.8767\n",
      "Epoch 25/30\n",
      " - 49s - loss: 0.0070 - accuracy: 0.9992 - val_loss: 0.6967 - val_accuracy: 0.8767\n",
      "Epoch 26/30\n",
      " - 49s - loss: 0.0146 - accuracy: 0.9967 - val_loss: 0.8137 - val_accuracy: 0.8500\n",
      "Epoch 27/30\n",
      " - 53s - loss: 0.0129 - accuracy: 0.9975 - val_loss: 0.8079 - val_accuracy: 0.8567\n",
      "Epoch 28/30\n",
      " - 57s - loss: 0.0132 - accuracy: 0.9975 - val_loss: 0.9344 - val_accuracy: 0.8533\n",
      "Epoch 29/30\n",
      " - 50s - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.8362 - val_accuracy: 0.8667\n",
      "Epoch 30/30\n",
      " - 49s - loss: 0.0086 - accuracy: 0.9983 - val_loss: 0.9140 - val_accuracy: 0.8500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Art     0.6522    0.8571    0.7407        70\n",
      "    Economic     0.9500    0.8636    0.9048        66\n",
      "    Politics     0.9545    0.6562    0.7778        64\n",
      "     Science     0.8710    0.9474    0.9076        57\n",
      "       Sport     1.0000    0.9767    0.9882        43\n",
      "\n",
      "    accuracy                         0.8500       300\n",
      "   macro avg     0.8855    0.8602    0.8638       300\n",
      "weighted avg     0.8736    0.8500    0.8519       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_RCNN = Build_Model_RCNN_Text(word_index,embeddings_index, 5)\n",
    "\n",
    "\n",
    "model_RCNN.summary()\n",
    "\n",
    "model_RCNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=30,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_RCNN.predict(X_test_Glove)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "target_names = ['Art', 'Economic', 'Politics', 'Science', 'Sport']\n",
    "report=metrics.classification_report(y_test, predicted, target_names=target_names, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
