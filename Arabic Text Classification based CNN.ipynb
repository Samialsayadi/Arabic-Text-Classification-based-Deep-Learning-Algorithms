{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense, GRU, Embedding\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, naive_bayes, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\n",
    "from keras.models import Sequential,Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\"cc.ar.50.vec\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text Classification based CNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "\n",
    "    \"\"\"\n",
    "        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "        word_index in word index ,\n",
    "        embeddings_index is embeddings index, look at data_helper.py\n",
    "        nClasses is number of classes,\n",
    "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
    "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "    # applying a more complex convolutional approach\n",
    "    convs = []\n",
    "    filter_sizes = []\n",
    "    layer = 5\n",
    "    print(\"Filter  \",layer)\n",
    "    for fl in range(0,layer):\n",
    "        filter_sizes.append((fl+2))\n",
    "\n",
    "    node = 128\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        #l_pool = Dropout(0.25)(l_pool)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
    "    l_cov1 = Dropout(dropout)(l_cov1)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
    "    l_cov2 = Dropout(dropout)(l_cov2)\n",
    "    l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "    l_flat = Flatten()(l_pool2)\n",
    "    l_dense = Dense(1024, activation='relu')(l_flat)\n",
    "    l_dense = Dropout(dropout)(l_dense)\n",
    "    l_dense = Dense(512, activation='relu')(l_dense)\n",
    "    l_dense = Dropout(dropout)(l_dense)\n",
    "    preds = Dense(nclasses, activation='softmax')(l_dense)\n",
    "    model = Model(sequence_input, preds)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(r\"aji-Arabic_corpus.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Corpus['text'],Corpus['targe'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54193 unique tokens.\n",
      "(1500, 500)\n",
      "Total 1999990 word vectors.\n",
      "Filter   5\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 500, 50)      2709700     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 499, 128)     12928       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 498, 128)     19328       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 497, 128)     25728       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 496, 128)     32128       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 495, 128)     38528       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 99, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 99, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 99, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 99, 128)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 99, 128)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 495, 128)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 491, 128)     82048       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 491, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 98, 128)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 94, 128)      82048       max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 94, 128)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 3, 128)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         394240      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            2565        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,924,041\n",
      "Trainable params: 3,924,041\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/15\n",
      " - 109s - loss: 1.7955 - accuracy: 0.2067 - val_loss: 1.6084 - val_accuracy: 0.2100\n",
      "Epoch 2/15\n",
      " - 94s - loss: 1.6204 - accuracy: 0.1875 - val_loss: 1.6069 - val_accuracy: 0.3000\n",
      "Epoch 3/15\n",
      " - 88s - loss: 1.6094 - accuracy: 0.2200 - val_loss: 1.6034 - val_accuracy: 0.3500\n",
      "Epoch 4/15\n",
      " - 143s - loss: 1.5990 - accuracy: 0.2492 - val_loss: 1.5959 - val_accuracy: 0.2167\n",
      "Epoch 5/15\n",
      " - 88s - loss: 1.5389 - accuracy: 0.3342 - val_loss: 1.5350 - val_accuracy: 0.5133\n",
      "Epoch 6/15\n",
      " - 69s - loss: 1.2904 - accuracy: 0.4592 - val_loss: 1.2745 - val_accuracy: 0.5933\n",
      "Epoch 7/15\n",
      " - 69s - loss: 0.9285 - accuracy: 0.6108 - val_loss: 1.0800 - val_accuracy: 0.5767\n",
      "Epoch 8/15\n",
      " - 123s - loss: 0.6162 - accuracy: 0.7317 - val_loss: 0.7537 - val_accuracy: 0.7967\n",
      "Epoch 9/15\n",
      " - 102s - loss: 0.3163 - accuracy: 0.8792 - val_loss: 0.4700 - val_accuracy: 0.8900\n",
      "Epoch 10/15\n",
      " - 69s - loss: 0.1414 - accuracy: 0.9417 - val_loss: 0.3422 - val_accuracy: 0.9400\n",
      "Epoch 11/15\n",
      " - 65s - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.2908 - val_accuracy: 0.9333\n",
      "Epoch 12/15\n",
      " - 77s - loss: 0.0215 - accuracy: 0.9967 - val_loss: 0.2266 - val_accuracy: 0.9300\n",
      "Epoch 13/15\n",
      " - 66s - loss: 0.0136 - accuracy: 0.9983 - val_loss: 0.2009 - val_accuracy: 0.9567\n",
      "Epoch 14/15\n",
      " - 61s - loss: 0.0109 - accuracy: 0.9983 - val_loss: 0.1778 - val_accuracy: 0.9533\n",
      "Epoch 15/15\n",
      " - 61s - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.1887 - val_accuracy: 0.9433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        62\n",
      "           1       0.93      0.89      0.91        61\n",
      "           2       0.81      0.93      0.87        60\n",
      "           3       1.00      0.98      0.99        59\n",
      "           4       1.00      1.00      1.00        58\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.95      0.94      0.95       300\n",
      "weighted avg       0.95      0.94      0.94       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "\n",
    "\n",
    "\n",
    "model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 5)\n",
    "\n",
    "\n",
    "model_CNN.summary()\n",
    "\n",
    "model_CNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=15,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_CNN.predict(X_test_Glove)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
